import os
from dotenv import load_dotenv, find_dotenv
import zhipuai
from langchain_core.outputs import LLMResult, Generation, RunInfo
from zhipuai_llm import ZhipuAILLM


def createPrompt(role, content, text=[]):
    # role æ˜¯æŒ‡å®šè§’è‰²ï¼Œcontent æ˜¯ prompt å†…å®¹
    jsoncon = {}
    jsoncon["role"] = role
    jsoncon["content"] = content
    text.append(jsoncon)
    return text


def get_zhipu_content_o(content: str, temperature: float = 0.0, max_tokens: int = 150, model: str = "chatglm_std"):
    question = createPrompt("user", content)
    response = zhipuai.model_api.invoke(
        model=model,
        prompt=question,
        temperature=temperature
    )
    ''' response æ ·å¼ï¼š
    {
        'code': 200,
        'msg': 'æ“ä½œæˆåŠŸ',
        'data': {
            'request_id': '8023318729410566227',
            'task_id': '8023318729410566227',
            'task_status': 'SUCCESS',
            'choices': [{
                'role': 'assistant',
                'content': '" ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ æ™ºè°±æ¸…è¨€ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"'
            }],
            'usage': {
                'prompt_tokens': 2,
                'completion_tokens': 28,
                'total_tokens': 30
            }
        }, 
        'success': True
    }
    '''
    return response["data"]["choices"][0]["content"]


def get_zhipu_content_l():
    # ä» env æ–‡ä»¶ä¸­è·å– zhipuai çš„ api_key å¹¶é…ç½®
    zhipuai_model = ZhipuAILLM(model="chatglm_std", temperature=0, zhipuai_api_key=os.getenv("ZHIPUAI_API_KEY"))
    zhipuai_model.generate(['ä½ å¥½'])
    LLMResult(generations=[[Generation(
        text='" ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ æ™ºè°±æ¸…è¨€ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"', generation_info=None)]],
        llm_output=None, run=[RunInfo(run_id='36840571-ce83-4bcb-8095-a222d59f32a4')])
